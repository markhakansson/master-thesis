\section{Overview}
RAUK is a command-line interface (CLI) tool that can be run on an existing
RTIC application with minimal changes to the source code in order to run a
schedulability analysis on the user tasks. During runtime, it automatically
patches in custom forks of certain libraries that are used by all RTIC
applications, to create custom builds of the RTIC application it is working on.
The forks are used to set up and configure the application required for RAUK to
get the necessary data to complete a schedulability analysis for the system.

Internally RAUK compiles the RTIC application to two \emph{harnesses} during
its runtime, a test harness and a replay harness. The harnesses are built in a
patched custom fork of RTIC and lets RAUK control the execution flow of the
RTIC application in order to generate tests via KLEE and replay each test on
the target hardware.

\subsection{Execution flow}
In RAUK there are four subcommands that need to be run in sequence.

\begin{enumerate}
    \item \texttt{generate}
    \begin{itemize}
        \item [--] Builds the test harness and generates test vectors on the test
            harness with KLEE
    \end{itemize}
    \item \texttt{flash}
    \begin{itemize}
        \item [--] Builds and flashes the replay harness to the target hardware
    \end{itemize}
    \item \texttt{measure}
    \begin{itemize}
        \item [--] Measures execution times on the replay harness on the target
            hardware using the generated test vectors
    \end{itemize}
    \item \texttt{analyze}
    \begin{itemize}
        \item [--] Runs a schedulability analysis on the measurement results
    \end{itemize}
\end{enumerate}

The reason for splitting the RAUK functionality into multiple subcommands is to
mainly separate the custom builds of the RTIC application from the actual
measuring and analysis on the hardware. The user might want to re-run the latter
commands multiple times and re-compiling the builds will be quite
time-consuming. The first two subcommands have been separated because they
build two different harnesses for two different target architectures and also
because the \texttt{flash} command needs access to the target hardware. This
way users who only wants to test their application for example, for errors can do so by
only running the \texttt{generate} subcommand.

\section{Components}

\subsection{Test harness}
The test harness used in RAUK is created in a modification of the RTIC library,
introduced as a patched fork in the RTIC application to be tested. It builds
similarly to the original application, but with modifications to enable KLEE to
analyze the code and generate test vectors for all user tasks in the
application.

The test harness is compiled to LLVM IR for the \texttt{x86--64} architecture
because KLEE currently only supports it. Because of this, a few embedded Rust support
libraries for the Cortex-M architecture have been modified to not call certain
Cortex-M specific instructions as KLEE does not support them and will throw
errors when encountering them.

Similarly to the approach by~\cite{lindner}, in the main function of the
application each user task will be assigned a concrete number inside a match
statement (Rust equivalent to a switch statement). Inside each match arm the
resources accessed by the corresponding task will be made symbolic and then
the task dispatcher or task handler connected to the task will be called. The
value controlling the flow of the match statement will also be made symbolic.
The difference to Lindner et al.\ is that before setting any resource as
symbolic, RAUK will test if the resource type can be set as symbolic. It
will test if the resources are of primitive types which can have a value at
compile time, such as integers and strings etc.

A simplified version of the test harness can be seen in Listing
~\ref{lst:testharness}.

\lstinputlisting[
    language={rust},
    label={lst:testharness},
    float=h,
    caption={Simplified test harness example}
]{../code/impl/test_harness.rs}

Late resources such as hardware peripherals that needs to be initialized at
run-time will be ignored inside the test harness as they can't be set as
symbolic because KLEE will not be able to infer their types. Therefore, whenever
a read is made on a hardware peripheral, the actual primitive value that is
returned will be made symbolic instead inside a patched fork of the
\texttt{vcell} library. In embedded Rust, as long as peripherals or memory
addresses are accessed through any of the abstractions available (which is
generally the case as it is recommended) then all read and writes will pass
through the library \texttt{vcell} which wraps around raw pointers to memory
addresses. Writes to any hardware memory addresses will not be made symbolic as
it will not affect the RTIC application's execution flow, i.e.\ it does not
have any side effects to the application. Although it is possible that some
writes on certain memory addresses can lead to hardware problems, but that is
beyond the scope of this thesis to test or verify.

A flowchart of the \texttt{generate} subcommand's execution flow, which creates
the test harness can be seen in Figure~\ref{fig:generatecmd}.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node (generate) [orangerectangle,rounded corners]
        {\texttt{generate}};
        \node (patch) [orangerectangle,fill=green!30,below of=generate]
        {Patch custom forks};
        \node (compile) [orangerectangle,fill=green!30,below of=patch]
        {Compile test harness to LLVM IR};
        \node (klee) [orangerectangle,fill=green!30,below of=compile]
        {Run KLEE on LLVM IR};

        \draw [arrow] (generate) -- (patch);
        \draw [arrow] (patch) -- (compile);
        \draw [arrow] (compile) -- (klee);

    \end{tikzpicture}
    \caption{Flowchart of the test generation subcommand}
    \label{fig:generatecmd}
\end{figure}

\subsection{Replay harness}
The replay harness is used by RAUK to help measure the execution times of all
user tasks on the target hardware using the previously generated test vectors.
The replay harness works similarly to the test harness. It is built in the same
custom RTIC fork and also wraps all user tasks inside a match statement with
the same number for each user task in the main function. However, it does not
set anything as symbolic and it will be compiled to a binary built for the
target hardware instead of to LLVM IR\@.

Since it is supposed to run on the target hardware, it also executes the RTIC
initialization function to set up the hardware correctly as the user intended
but disables interrupts such that no task will be unexpectedly executed during
runtime. In addition, the replay harness enables the Data Watchpoint and Trace
Unit (DWT) on the Cortex-M hardware in order for RAUK to be able to fetch the
current cycle counter at any time.

The aforementioned match statement is wrapped in a never-ending loop with a
software breakpoint before the match statement. This will let RAUK know when a
new test vector should be measured. Once this breakpoint is reached during
execution, the next test vector to measure will be written to memory,
overriding all variables that KLEE generated tests for (sans \texttt{vcell}
readings). Then by resuming execution one of the match arms will be targeted
and a new measurement for a task can begin.

To enable RAUK to measure the execution time of all user tasks using the
generated test vectors, there will be software breakpoints inserted before and
after each task's execution. As well as before the claim and after the release
of a resource. When reaching these breakpoints the current cycle counter can be
recorded. There is also an additional breakpoint inside the patched
\texttt{vcell} library that is set when a memory address is read and a breakpoint
inserted inside an executing task and inside a resource lock. The former is
used to notify RAUK to override the returned value of a hardware reading with
the test vector value. The latter two are used by RAUK to help label the
traces correctly.

\lstinputlisting[
    language={rust},
    label={lst:replayharness},
    caption={Simplified replay harness example}
]{../code/impl/replay_harness.rs}

Each breakpoint will be given an immediate value as an argument depending on
where it is placed in the replay harness. When running the replay harness with
the generated test vectors, the application will halt at each breakpoint. RAUK
can then successively try to create a correct execution trace by stepping
through the application as it will know what type of breakpoint it is by reading
the immediate value. A simplified code example of the replay harness can be
seen in Listing~\ref{lst:replayharness}.

The execution flow for the \texttt{flash} subcommand which builds the replay
harness can be seen in Figure~\ref{fig:flashcmd}.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node (flash) [orangerectangle,rounded corners]
        {\texttt{flash}};
        \node (patch) [orangerectangle,fill=green!30,below of=flash]
        {Patch custom forks};
        \node (compile) [orangerectangle,fill=green!30,below of=patch]
        {Compile replay harness to binary};
        \node (hw) [orangerectangle,fill=green!30,below of=compile]
        {Flash binary to connected hardware};

        \draw [arrow] (flash) -- (patch);
        \draw [arrow] (patch) -- (compile);
        \draw [arrow] (compile) -- (hw);

    \end{tikzpicture}
    \caption{Flowchart of the flash subcommand}
    \label{fig:flashcmd}
\end{figure}

\subsection{Measure of replay harness}
Once the replay harness has been flashed onto the target hardware, it can
be measured by RAUK from the PC connected to the target hardware. The main
objectives of the measure subcommand is to, for each test vector generated
\begin{itemize}
   \item Write the test vector results to the correct locations in flash or registers.
   \item Step through the breakpoints and record a trace for the current task.
\end{itemize}
From a single breakpoint a single trace will be recorded which contains the
type of the trace, the cycle count at the breakpoint and the name of the task
or resource where the breakpoint has been inserted into. This will be repeated
until all test vectors have been run once in the replay harness.

In order to generate a correct trace, the measure subcommand needs to do many
things. For each breakpoint it checks the breakpoint immediate value to
determine what kind of trace it is. If it is a task or a resource breakpoint, it
needs to record the current clock cycle at the breakpoint and also figure out
what the name of the object is. To figure out the name of the task or resource
where the breakpoint has been inserted into, RAUK reads the
DWARF\cite{dwarfspec} debug data of the flashed binary. DWARF debug data
contains application information useful for debugging, such as memory
locations of variables and information about which instructions in the binary
belongs to what functions etc.

%\begin{figure}
%    \centering
%    \begin{tikzpicture}[node distance=2cm]
%        \node (measure) [redrectangle]
%        {\texttt{measure}};
%        \node (probe-rs) [orangerectangle,below of=measure, xshift=-4cm]
%        {\texttt{probe-rs}};
%        %\node (chip) [orangerectangle,below of=probe-rs]
%        %{\texttt{chip}};
%        \node (trace) [orangerectangle,below of=measure]
%        {\texttt{trace}};
%        \node (utils) [orangerectangle,below of=measure, xshift=4cm]
%        {\texttt{binary utilities}};
%        \node (dwarf-parser) [orangerectangle, below of=utils,xshift=-2cm]
%        {\texttt{dwarf-parser}};
%        \node (disassembler) [orangerectangle, below of=utils,xshift=2cm]
%        {\texttt{disassembler}};
%
%        % midpoints for the lines
%        \node (f1) [below of=measure, yshift=1cm]{};
%        \node (f2) [below of=measure, yshift=1cm, xshift=-4cm]{};
%        \node (f3) [below of=measure, yshift=1cm, xshift=4cm]{};
%
%        \node (f4) [below of=utils, yshift=1cm]{};
%        \node (f5) [below of=utils, yshift=1cm, xshift=-2cm]{};
%        \node (f6) [below of=utils, yshift=1cm, xshift=2cm]{};
%
%
%        \draw (flash) -- (trace);
%        \draw (f1.center) -- (f2.center);
%        \draw (f2.center) -- (probe-rs);
%        %\draw (probe-rs) -- (chip);
%
%        \draw (f1.center) -- (f3.center);
%        \draw (f3.center) -- (utils);
%        \draw (utils) -- (f4.center);
%        \draw (f4.center) -- (f5.center);
%        \draw (f4.center) -- (f6.center);
%        \draw (f5.center) -- (dwarf-parser);
%        \draw (f6.center) -- (disassembler);
%    \end{tikzpicture}
%    \caption{Measure subcommand}
%    \label{fig:measureparts}
%\end{figure}

If the RTIC application reads any hardware peripherals or memory addresses
during its runtime, there will be test vectors generated for those reads. For
primitive resources, they are set as global variables and usually have an
associated memory address in flash and are easy to overwrite. But for direct
memory readings, they are read directly into a register on the target hardware.
Therefore, the test vectors for memory reads needs to overwrite the register
that the actual value is loaded into.

This is done by inserting a breakpoint inside
the patched fork of the \texttt{vcell} library, where a memory read is made.
Once reaching that breakpoint and by disassembling the binary, RAUK checks
which register the read of the memory address is loaded into. Then sets a
hardware breakpoint after the load has been made. A hardware breakpoint
can be set at runtime and does not induce any overhead. This is because the
hardware itself handles the breakpoint. When reaching that hardware breakpoint
RAUK will overwrite the register with the corresponding test vector and resume
the program execution.

Reading the DWARF and the disassembled instructions will not induce any
overhead on the actual analysis results, as that is done on the built binary
directly on the PC and not on the flashed binary on the target hardware.

After completing all traces, RAUK will convert them to a new set of traces where
there is a single trace for each test vector tested. Which importantly contains
the cycle count at the start and end of the trace. The complete contents of a
final trace can be seen in Table~\ref{tab:tracecontents}.
\begin{table}[h]
    \centering
    \begin{tabular}{|c | c|}
        \hline
        \multicolumn{2}{| c |}{Trace contents}\\ [0.5ex]
        \hline
        Label & Description\\ [0.5ex]
        \hline
        \texttt{name} & Name of the object trace  \\
        \hline
        \texttt{ttype} & The type of trace (task, resource lock etc.) \\
        \hline
        \texttt{start} & Cycle count at the start of this trace  \\
        \hline
        \texttt{inner} & List of traces inside this trace \\
        \hline
        \texttt{end} & Cycle count at the end of this trace  \\
        \hline
    \end{tabular}
    \caption{Description of the final trace data.}
    \label{tab:tracecontents}
\end{table}


\subsection{Schedulability analysis}
After collecting the traces, the final step is to run the schedulability analysis as
detailed in Section~\ref{theory:schedulability}. Assuming that the traces are correct,
and they contain the WCET of all tasks. The \texttt{analysis} subcommand needs
additional information about each user task to complete the analysis, which can
be supplied via a TOML file. This file needs to specify for each user task,
\begin{itemize}
    \item The name of the task
    \item The task priority
    \item The relative deadline
    \item The period (inter-arrival) of the task.
\end{itemize}

Then from the list of traces in the previous step, the WCET for each task will
be found and the WCRT for each task will be calculated from them. RAUK will
then confirm whether the system is schedulable by comparing the deadlines to
the calculated WCRT and check if the calculated system load does not exceed
100\%.
