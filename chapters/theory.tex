This chapter details the necessary thoery that was required in the thesis.
It will be explained from the onset of this thesis. Theory and minor details
that are not applicable has been intentionally left out.

\section{Stack Resource Policy}
\label{theory:srp}
Stack Resource Policy (SRP) is a policy that defines how shared resources
should be accessed in a system, to prevent both deadlocks and multiple priority
inversions \cite{srp}. A \emph{priority inversion} is where a higher priority
task $\alpha$ is unable to continue execution as its waiting for a lower
priority task $\beta$ to finish executing. Once a task has access to a
resource, it cannot be blocked by another task. Following SRP context
switching (changing execution from a task to another) is limited. This is
because a task will be blocked from executing at the time when it starts to
context switch, if it will access resources that it would otherwise be blocked
at when acquiring them. Because of the early blocking, tasks do not need their
own private stack space but can share a single stack during execution. Using a
single stack makes SRP very memory efficient compared to conventional operating
systems\cite{hardrealtimecomputingsystems}.

SRP is not a scheduler but an extension to current schedulers. It works with
schedulers with either fixed or dynamic priorities on tasks such as rate
monotonic (RM) or earliest-deadline first (EDF).

\subsection{Definitions}
\label{theory:srp:definitions}
Here follows some definitions used in SRP required to understand the
schedulability analysis in later sections and chapters.

\subsubsection{Preemption level}
\label{theory:srp:definitions:preemption}
\emph{Preemption levels} are defined for all tasks in SRP and are static
values, used to check for potential \emph{blocking} when SRP is used with
dynamic priorities of tasks.  Because RTIC uses fixed priorities, the
preemption level $\pi_i$ for a task $\tau_i$ is the same as the task's priority
$P_{\tau_i}$.
\begin{equation}
    \pi_i = P_{\tau_i}
\end{equation}
Priority and preemption level will be used interchangeably in this section.

\subsubsection{Resource ceiling}
\label{theory:srp:definitions:resource}
In SRP each resource $R_k$ have an associated \emph{resource ceiling}
$C_{R_k}$. The definition of the resource ceiling is left abstract in the
paper, it only needs to follow certain conditions which will not be explained
here. In RTIC this ceiling is set to be
\begin{equation}
    C_{R_k} = \max\{P_{\tau_i}\}
\end{equation}
for all $i$ number of tasks accessing the resource $R_k$. Or in order words,
the resource ceiling for $R_k$ is equal to the priority of the task with the
highest priority accessing the resource $R_k$.

\subsubsection{Blocking time}
\label{theory:srp:definitions:blocking}
A higher priority task $\tau_i$ is blocked from accessing a resource $R_k$ by
another lower priority task $\tau_j$, if $\pi_j < \pi_i$ and $C_{R_k} \geq
\pi_i$.

In SRP, a task cannot be blocked by other tasks for more than a single
\emph{critical section} during its execution. A critical section is where a
shared resource is subjected to mutual exclusion. I.e.\ where a shared resource
can be acquired and the acquirer has exclusive access to it, blocking other
tasks from acquiring it until the acquirer releases the resource.

If the duration of the longest critical section of a task $\tau_j$ on resource
$R_k$ is denoted by $\delta_{j,k}$, then the maximum blocking time $\tau_i$ can
be subjected to is defined as follows
\begin{equation}
    B(\tau_i) = \max\{\delta_{j,k}\}
\end{equation}
for all tasks $\tau_j$ accessing resource $R_k$ where $\pi_j < \pi_i$ and
$C_{R_k} \geq \pi_i$.
% mention:
% - preemption
% -

\section{RTIC}
Real-Time Interrupt-driven Concurrency or RTIC for short, is a concurrency
framework for embedded systems based on the Stack Resource Policy for
scheduling tasks. It based on the RTFM framework\cite{rtfm}. It abstracts many
details of the underlying model from its users without any significant overhead
and a guaranteed deadlock-free execution. In RTIC the user defines the
application by defining the shared resources, the initialization function and
the \emph{tasks} in the system. The initialization function is the first
function to run in the application. It has complete access to the hardware and
is used to initialize the application and the hardware itself, as well as
initializing any shared resources that can only be set during run-time.

A \emph{task} is the unit of concurrency used within RTIC. It can be bound to
an interrupt vector, then it becomes a \emph{hardware task}. Which will be
triggered on a certain hardware event. Or not, then it is a \emph{software
task}. Which can be triggered inside the application itself. All tasks can be
prioritized and the priorities will be fixed. The scheduler will schedule
the task with the highest priority of the tasks that are ready to be executed.
Known as fixed-priority pre-emptive scheduling\cite{fixedpriorityhistory}.

An example of shared resources in an RTIC application can be seen in Listing
\ref{lst:shareresources}. Here the \texttt{integer} is set to be initialized to
$0$ at compile-time whereas the \texttt{input} input pin will have to be initialized
correctly at run-time.
\lstinputlisting[
    language={rust},
    label={lst:shareresources},
    float=h,
    caption={RTIC shared resources}
]{../code/rtic_resources.rs}

A task accessing the shared resources can be seen in Listing \ref{lst:task}.
All shared resources that tasks needs access to will have to be declared in the
task attributes. To get access to the shared resource \texttt{input} inside the
task it needs to be locked. Once a resource is locked it will have exclusive
access to it.
\lstinputlisting[
    language={rust},
    label={lst:task},
    float=h,
    caption="RTIC task"
]{../code/rtic_task.rs}

When the user compiles their RTIC application the application will be expanded
with all necessary details. Using Rust's procedural macros. It will take the
information in the attributes and create helper functions for all tasks and
resources and connect tasks with dispatchers. As well as functions for updating
resource priorities which is required to adhere to the stack resource policy.

RTIC has very little overhead on the application. This is because it lets the
hardware itself handle most of the scheduling work using ARM's nested vector
interrupt controller (NVIC).  The overhead RTIC has mostly concerns context switching
as well as task dispatching.

\section{Schedulability Analysis}
For a system with fixed priority preemptive scheduling, the response time for
a task $\tau_i$ can be calculated from the recurrence relation by Audsley et
al.\ \cite{audsley93} and extended with the blocking time for the
task as per\cite{hardrealtimecomputingsystems} as follows
\begin{equation}
    \begin{cases}
        R_{i}(0) &= C_i + B_i \\
        R_{i}(s) &= C_i + B_i + \sum\limits_{h: P_h > P_i} \left\lceil \frac{R_{i}(s-1)}{T_h} \right\rceil.
    \end{cases}
\end{equation}
Where $T_h$ is the period of a periodic task $\tau_h$ or expected inter-arrival
time for a non-periodic task $\tau_h$. The third term (the summation) in the recurrence
relation is the possible preemption from higher priority tasks that can occur.


A system is said to be schedulable if all tasks $\tau_i$ are responding within their respective
deadlines $D_{\tau_i}$. Thus an RTIC system will be schedulable if the following holds
true
\begin{equation}
    R_{\tau_i} \leq D_{\tau_i}, \quad \forall i.
\end{equation}

\section{LLVM}
LLVM is a compiler technology toolchain which can be used by programming
languages to produce machine code. It can be used from inside a compiler in a
programming language to produce LLVM intermediate representation code (LLVM IR)
which is used internally in LLVM to run optimizations and produce machine code
for any supported instruction sets.

It is used by the Rust language (which RTIC is developed for) as its backend.
The compilation pipeline for the Rust language starts with translating the
source code to the Rust compiler's internal immediate representation to
verify the program and run some optimzation. Then it uses LLVM as the backend
to create a binary for the target architecture. The pipeline for creating
an ARM binary can be seen in Figure \ref{figure:rustcompilation}.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node (rustsource) [orangerectangle] {Source code};
        \node (hir) [orangerectangle, below of=rustsource] {High-level IR};
        \node (mir) [orangerectangle, below of=hir] {Mid-level IR};
        \node (llvmir) [redrectangle, below of=mir] {LLVM IR};
        \node (armbinary) [orangerectangle, below of=llvmir] {ARM machine code};

        \draw [arrow] (rustsource) -- (hir);
        \draw [arrow] (hir) -- (mir);
        \draw [arrow] (mir) -- (llvmir);
        \draw [arrow] (llvmir) -- (armbinary);
    \end{tikzpicture}
    \caption{Rust language compilation pipeline}
    \label{figure:rustcompilation}
\end{figure}


\section{KLEE - symbolic execution engine}
KLEE\cite{kleepaper} is a tool to generate test vectors for programs with the
ability to replay them, using symbolic execution. Compared to fuzz testing
where program input usually consists of randomized concrete values, symbolic
execution handles input as symbolic values which are arbitrary. These values
are handled by an interpreter which executes the program and keeps track of the
states where the symbolic values are used for branching paths (e.g.\
conditional branches such as `if' and `else') and modifications to and/or using
those values. Then for each state it utilizes an SMT solver to find for which
values the symbolic values should be in order to enter those branches or
invalid modifications. This could be the case where an array is accessed using
a symbolic value as the index. The SMT solver will find that the array will be
accessed out of bonds for certain values of it. Thus producing two branches,
one where the array is not accessed out of bonds and one where it is. As the
symbolic value changes over the program, the interpreter creates a tree for all
possible branching paths of its value. Finally when the program terminates KLEE
will output the test vectors and the user can replay those test vectors on the
program.
