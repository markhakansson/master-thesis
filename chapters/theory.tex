This chapter details the necessary theory that is required to understand this thesis.
Minor details that are not applicable have been intentionally left out.

\section{Stack Resource Policy}\label{theory:srp}
Stack Resource Policy (SRP) is a policy that defines how shared resources
should be accessed in a system, to prevent both deadlocks and multiple priority
inversions~\cite{srp}. A \emph{priority inversion} is where a higher priority
task $\tau_\alpha$ is unable to continue execution as it's waiting for a lower
priority task $\tau_\beta$ to finish executing. Once a task has access to a
resource, it cannot be blocked by another task. Following SRP, context
switching (changing execution from a task to another) is limited. This is
because a task will be prevented from executing before it starts to context
switch, if it will later access resources where it will be blocked at when
acquiring them. Because of the early blocking, tasks do not need their own
private stack space but can share a single stack during execution. Using a
single stack makes SRP very memory efficient compared to conventional operating
systems\cite{hardrealtimecomputingsystems}.

SRP is not a scheduler but an extension to current schedulers. It works with
schedulers with either fixed or dynamic priorities on tasks, such as rate
monotonic (RM) or earliest-deadline first (EDF).

\subsection{Definitions}\label{theory:srp:definitions}
Here follows some definitions used in SRP required to understand the
schedulability analysis in later sections and chapters.

\subsubsection{Preemption level}\label{theory:srp:definitions:preemption}
\emph{Preemption levels} are defined for all tasks in SRP and are static
values, used to check for potential \emph{blocking} when SRP is used with
dynamic priorities of tasks. Because RTIC uses fixed priorities, the
preemption level $\pi_i$ for a task $\tau_i$ is the same as the task's priority
$P_{\tau_i}$
\begin{equation}
    \pi_i = P_{\tau_i}.
\end{equation}
Priority and preemption level will be used interchangeably in this section.

\subsubsection{Resource ceiling}\label{theory:srp:definitions:resource}
In SRP each resource $R_k$ have an associated \emph{resource ceiling}
$C_{R_k}$. The definition of the resource ceiling is left abstract in the
paper\cite{srp}: it only needs to follow certain conditions which will not be
explained here. In RTIC this ceiling is set to be
\begin{equation}
    C_{R_k} = \max\{P_{\tau_i}\}
\end{equation}
for all tasks $\tau_i$ accessing the resource $R_k$. Or in other words,
the resource ceiling for $R_k$ is equal to the priority of the task with the
highest priority accessing the resource $R_k$.

\subsubsection{Blocking time}\label{theory:srp:definitions:blocking}
A higher priority task $\tau_i$ is said to be blocked from accessing a resource
$R_k$ by a critical section belonging to another lower priority task $\tau_j$,
if task $\tau_j$ is claiming the resource $R_k$ where $\pi_j < \pi_i$ and
$C_{R_k} \geq \pi_{\tau_i}$.

In SRP, a task cannot be blocked by other tasks for more than a single
\emph{critical section} during its execution. A critical section is where a
shared resource is subjected to mutual exclusion. I.e.\ where a shared resource
can be acquired and the acquirer has exclusive access to it, blocking other
tasks from acquiring it until the acquirer releases the resource.

If the duration of the longest critical section of a task $\tau_j$ on resource
$R_k$ is denoted by $\delta_{j,k}$, then the maximum blocking time $B_i$
that $\tau_i$ can be subjected to is defined as follows
\begin{equation}
    B_i = \max\{\delta_{j,k}\}
\end{equation}
for all tasks $\tau_j$ accessing resource $R_k$ where $\pi_j < \pi_i$ and
$C_{R_k} \geq \pi_{\tau_i}$.

\subsubsection{System ceiling}\label{theory:srp:definitions:system}
A system-wide ceiling called the \emph{system ceiling} is defined as the
largest resource ceiling of the resources that are currently claimed by
any task under mutual exclusion. The equation for the system ceiling is
\begin{equation}
    \Pi = \max\{C_{R_l}\}
\end{equation}
for all currently claimed resources $R_l$.

\subsubsection{Preemption test}
The following test from~\cite{hardrealtimecomputingsystems} p. 238, is what
stops more than one priority inversion from occurring in SRP
\begin{displayquote}
    “SRP Preemption Test: A task is not permitted to preempt until its priority
    is the highest among those of all the tasks ready to run, and its preemption
    level is higher than the system ceiling.”
\end{displayquote}
This should be tested each time after a resource has been released by a task.

\section{RTIC framework}
Real-Time Interrupt-driven Concurrency or RTIC for short, is a concurrency
framework for embedded Rust systems following the Stack Resource Policy for
sharing resources. It is based on the RTFM framework\cite{rtfm} and currently
supports all ARM Cortex-M processors. During the compilation of an RTIC
application, RTIC analyzes the set of tasks and resources in the application to
generate a binary that exploits the target hardware for a efficient resource
handling. RTIC manages this without any significant overhead in the rendered
binary, and a comes with a guaranteed deadlock-free execution during the
compilation stage and with a data race free memory sharing during runtime. 

In RTIC the user defines the application by defining the shared resources, the
initialization function, and the \emph{user tasks} in the system. The
initialization function is the first function to run in the application. It has
complete access to the hardware and is used to initialize the application and
the hardware itself, as well as initializing any shared resources that can only
be set during run-time.

A \emph{task} is the unit of concurrency used within RTIC\@. If bound to an
interrupt vector, it becomes a \emph{hardware task}. Which will be
triggered on a certain hardware event. If not bound, it is a \emph{software
task} which can be triggered inside the application itself. All tasks can
be prioritized and the priorities are fixed. The scheduler will schedule the
task with the highest priority of the tasks that are ready to be executed.
Known as fixed-priority pre-emptive scheduling\cite{fixedpriorityhistory}.

An example of shared resources in an RTIC application can be seen in Listing
~\ref{lst:shareresources}. Here the \texttt{integer} is set to be initialized to
$0$ at compile-time whereas the \texttt{input} input pin will have to be initialized
correctly at run-time.
\lstinputlisting[
    language={rust},
    label={lst:shareresources},
    float=h,
    caption={Example of RTIC shared resources.}
]{../code/rtic_resources.rs}

A task accessing the shared resources can be seen in Listing~\ref{lst:task}.
All shared resources that tasks needs access to will have to be declared in the
task attributes. To get access to the shared resource \texttt{input} inside the
task it needs to be locked. Once a resource is locked it will have exclusive
access to it.
\lstinputlisting[
    language={rust},
    label={lst:task},
    float=h,
    caption={Simplified RTIC task.}
]{../code/rtic_task.rs}

When the user compiles their RTIC application, the application will be expanded
with all necessary details needed using Rust's procedural macros. RTIC analyzes
the information in the RTIC application to create helper functions for all
tasks and resources, which are  used to connect tasks to task dispatchers or
task handlers. Here, RTIC also adds functions for updating the resource
priorities which is required in order to adhere to the Stack Resource Policy.
Hardware tasks will be bound to their own interrupt handler and software tasks
are dispatched via one or more task queue dispatcher each bound to a single
interrupt. Therefore, software tasks with the same priority will share the same
dispatcher, which is bound to a single interrupt.

RTIC has very little overhead on the application. This is because it lets the
hardware itself handle most of the scheduling work using ARM's nested vector
interrupt controller (NVIC). The overhead RTIC has mostly concerns context switching
as well as task dispatching.

\section{Schedulability analysis techniques}\label{theory:schedulability}
For a system with fixed priority preemptive scheduling, the response time $R_i$
for a task $\tau_i$ can be calculated from the recurrence relation by Audsley
et al.\ \cite{audsley93} and extended with the blocking time for the task as
per\cite{hardrealtimecomputingsystems}
\begin{equation}
    \begin{cases}
        R_{i}(0) &= C_i + B_i \\
        R_{i}(s) &= C_i + B_i + \sum\limits_{h: P_h > P_i} \left\lceil \frac{R_{i}(s-1)}{T_h} \right\rceil.
    \end{cases}
\end{equation}
Where $T_h$ is the period of a periodic task $\tau_h$ or expected minimal
inter-arrival time for a non-periodic task $\tau_h$. $C_i$ is the WCET and
$B_i$ the blocking time of the task. The third term (the summation) in the
recurrence relation is the possible preemption from higher priority tasks that
can occur.

\subsection{Worst-case total system load}
If the tasks are to be schedulable in the system, they will also need to not
utilize more than the available processor time. The worst-case total system
load that the tasks can subject the system (i.e.\ the processor) to is defined
as per~\cite{hardrealtimecomputingsystems}

\begin{equation}
    \rho = \sum^{n}_{i=1} \frac{C_i}{T_i}
\end{equation}
where $n$ is the number of tasks, $C_i$ the WCET and $T_i$ the
minimal period that the task $\tau_i$ is expected to inter-arrive at.

\subsection{Schedulability check}
A system is said to be schedulable if all tasks $\tau_i$ are responding within
their respective relative deadlines $D_{\tau_i}$ and the system load is not exceeding
100\% at any time. A relative deadline in this context is the deadline counted
from when the task was requested.

Thus, an RTIC system will be schedulable if the following holds true
\begin{equation}
    R_{\tau_i} \leq D_{\tau_i}, \quad \forall i
\end{equation}
and
\begin{equation}
    \rho \leq 1.0.
\end{equation}


\section{LLVM}
LLVM is a compiler technology toolchain which can be used by programming
languages to produce machine code (among many other features). It can be used
from inside a compiler in a programming language to produce LLVM intermediate
representation code (LLVM IR) which is used internally in LLVM to run
optimizations and produce machine code for any supported instruction sets.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node (rustsource) [orangerectangle] {Source code};
        \node (hir) [orangerectangle, below of=rustsource] {High-level IR};
        \node (mir) [orangerectangle, below of=hir] {Mid-level IR};
        \node (llvmir) [redrectangle, below of=mir] {LLVM IR};
        \node (armbinary) [orangerectangle, below of=llvmir] {ARM machine code};

        \draw [arrow] (rustsource) -- (hir);
        \draw [arrow] (hir) -- (mir);
        \draw [arrow] (mir) -- (llvmir);
        \draw [arrow] (llvmir) -- (armbinary);
    \end{tikzpicture}
    \caption{Rust language compilation pipeline.}
    \label{figure:rustcompilation}
\end{figure}

It is used by the Rust language (which RTIC is developed for) as its backend.
The compilation pipeline for the Rust language starts with translating the
source code to the Rust compiler's internal immediate representation to
verify the program and apply some optimizations. Then it uses LLVM as the backend
to create a binary for the target architecture. The pipeline for creating an
ARM binary from Rust source code can be seen in Figure
~\ref{figure:rustcompilation}.


\section{KLEE -- symbolic execution engine}
KLEE\cite{kleepaper} is a tool to symbolically generate test vectors on LLVM IR with the
ability to replay them using symbolic execution. Compared to fuzz testing
where program input usually consists of randomized concrete values, symbolic
execution handles input as symbolic values which are arbitrary and are instead
treated as boolean expressions. In KLEE the symbolic values are handled by an
interpreter which executes the program and keeps track of all the states where
the symbolic values are used for branching paths (e.g.\ conditional branches
such as `if' and `else') and modifications to and/or using those values. The
states will be recorded as a tree, where all paths will lead to new possible
states.  This interpreter will continue as long as there are possible states to
check.

In Section 3.1 at page 4 in~\cite{kleepaper} the authors describe how the
constraint solver is used inside the interpreter. For each state, KLEE utilizes
a satisfiability modulo theories (SMT) solver to determine if any branching
paths or invalid modifications are possible on the symbolic values. The SMT
solver checks if a boolean expression can be proven to be true, false or if it
is undecidable. On a conditional branch the SMT solver checks if the path
condition can be proven to be true or false. If proven true then only the
branching path needs to be checked. If proven false it can be ignored. Should
it be undecidable, then both branching paths are viable. An example of this
would be the case where a fixed-sized array is accessed using a symbolic value
as the index. There will be two branches, one where the array is accessed out
of bounds and one where it isn't. The SMT solver will not be able to prove that
any of these branches are viable or not. Thus, both paths are possible for the
symbolic value.

Finally, when the interpreter terminates, KLEE will generate concrete values as
test vectors from the states it has recorded: one test vector for each possible
path. The user can then use KLEE to replay the test vectors on the LLVM IR to hit
the same paths that created them in the application.

\subsection{Example}
Here follows a simple example of KLEE generating test vectors on a small Rust
function, covering all possible paths in it. It does not cover all checks that
KLEE makes, such as checking for index out of bounds or arithmetic overflow.

In Listing~\ref{lst:kleefunction} a function with multiple conditional branches
can be seen. It takes an integer as input and returns an integer. It will
panic inside the second and third conditional branches. A panic in Rust will
simply terminate the program immediately.
\lstinputlisting[
    language={rust},
    label={lst:kleefunction},
    float=h,
    caption={Function to test with KLEE.}
]{../code/klee_function.rs}

Before running KLEE on this function, the input needs to be set symbolically in
the source code as seen in Listing~\ref{lst:kleesymbolic}. After compiling this
program to LLVM IR, KLEE can pick up which variables have been set as symbolic
values and will know what variables to keep track of.
\lstinputlisting[
    language={rust},
    label={lst:kleesymbolic},
    float=h,
    caption={Marking the input to the function as symbolic.}
]{../code/klee_set.rs}

A simplification of the states or execution paths recorded, when executing KLEE
on the produced LLVM IR, can be seen in the tree in Figure
~\ref{figure:kleetree}.  One notable change is that the third conditional
branch, where \texttt{value} is compared against the number $5000$ has
completely disappeared. This is because the SMT solver has proven that the
symbolic value \texttt{value} cannot be smaller than 2000 and equal to 5000 at
the same time.  Therefore, that branch is completely disregarded by KLEE\@.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=2.5cm]
        \node (function) [orangerectangle] {function(input)};
        \node (first) [draw, diamond, below of=function, aspect=2, yshift=-5mm] {$value < 2000$};
        \node (complete_left) [redrectangle, below of=first, aspect=2, xshift=-3cm, fill=green!30] {complete};
        \node (increment) [redrectangle, below of=first, xshift=3cm, fill=blue!30] {$value + 1000$};
        \node (1234) [draw, diamond, below of=increment, aspect=2] {$value == 1234$};
        \node (panic) [redrectangle, below of=1234, aspect=2, xshift=3cm] {panic};
        \node (complete_right) [redrectangle, below of=1234, aspect=2, xshift=-3cm, fill=green!30] {complete};

        \draw [arrow] (function) -- node[fill=white]{value = input} (first);
        \draw [arrow] (first) -- node[fill=white]{false} (complete_left);
        \draw [arrow] (first) -- node[fill=white]{true} (increment);
        \draw [arrow] (increment) -- (1234);
        \draw [arrow] (1234) -- node[fill=white]{false} (complete_right);
        \draw [arrow] (1234) -- node[fill=white]{true} (panic);
    \end{tikzpicture}
    \caption{An example of the states recorded by KLEE from Listing~\ref{lst:kleefunction}.}
    \label{figure:kleetree}
\end{figure}
After checking all possible execution paths, KLEE will generate test vectors
for each path in the tree it has recorded as concrete values by backtracking.
As can be seen in Figure~\ref{figure:kleetree} there are three paths in total,
whereof one results in an error. KLEE will report paths that results in errors,
separately from regular paths. The generated test vectors can be seen in Table
~\ref{tab:kleeexample}.

\begin{table}[h]
    \centering
    \begin{tabular}{||c c c||}
        \hline
        Test case & Value of \texttt{input} & Result \\ [0.5ex]
        \hline\hline
        1 & 2000 & Success\\
        \hline
        2 & 0 & Success \\
        \hline
        3 & 234 & Error \\
        \hline
    \end{tabular}
    \caption{The test vectors generated by KLEE on Listing \ref{lst:kleefunction}.}
    \label{tab:kleeexample}
\end{table}

\subsection{Problems}
% Problem
A problem with KLEE and symbolic execution in general is the \emph{path
explosion} problem. For larger applications the paths recorded by KLEE will
increase substantially and for programs that never terminate, KLEE will
continue executing indefinitely unless a timeout has been specified.
