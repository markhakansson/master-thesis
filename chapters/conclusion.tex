The goal of this thesis was to explore the possibilities and limitations of
developing a tool to automatically calculate the WRCT for user tasks in an
RTIC application.

Inspired by the previous work by \cite{lindner} the thesis resulted in the tool
RAUK for response-time analysis of RTIC applications using the symbolic
execution tool KLEE for automatic WCET measurements. With minimal changes to
the RTIC application, RAUK uses KLEE to generate test vectors in order to
target all execution paths in the user tasks. Then the execution times for each
test vector can be replayed and measured on the target hardware. Using these
measurements a schedulability analysis check can be performed following the Stack
Resource Policy and the fixed-priority preemptive scheduling theory that RTIC
adheres to, in order to check whether the system is schedulable or not.

The tool was tested on an RTIC application which toggled an LED on the
target development board and could be interrupted externally via a button
on the board. Using RAUK the tool found one arithmetic error when the
application was executed without any optimizations and given some extra input,
the tool could determine whether the system was schedulable or not by comparing
the calculated WCRT for each task with their respective deadlines and that the
worst-cast total system load did not exceed 100\%.

With the current limitations RAUK adds some constant overhead to the measured
execution time results. For tasks with generally longer execution times, this
overhead will result in a small fraction of more cycles in the resulted
measurements. But for tasks with generally short execution times the resulted
measurements can be many times longer than the actual execution time.

\section{Future work}
Some solutions to the limitations and problems discussed in Chapter
\ref{chapter:discussion} will be addressed here. Which are also recommendations
for fixes in the next or a future iteration of RAUK.

\textbf{Better test vectors:} The problem with certain code sections being
optimized out when generating test vectors with compiler optimizations enabled
can be easily fixed by introducing side effects in the aforementioned
functions. In the current implementation when generating tests, the ARM related
instructions do nothing in order to make the application run on the Linux
x86-64 toolchain. The compiler notices this if it is told to optimize the code
and will simply remove these sections instead of keeping them in the built
binary.

Side effects can be added inside of these instructions by e.g.\ do a volatile
read to a memory address inside them. By reading or writing directly to memory
addresses, the Rust compiler will not optimize out that specific code sections
when the application is built with optimizations enabled. This is because the
compiler is unable to determine if volatile memory accesses have side effects
or not.

In this thesis the assumption that KLEE generates test vectors for the longest
path, was not proven. To combat this, the symbolic execution tool used in RAUK
would have to run on the hardware instructions instead of on LLVM IR. KLEE does
not support this however and only runs on top of LLVM IR. Or alternatively one
could investigate how LLVM IR is translated into ARM instructions to see what
relations there are. E.g. to see if some LLVM instructions are translated to
fewer or more equivalent ARM instructions. Using this information it could
be possible to determine whether the test vectors generated, in practice
actually contain a longest path or not on the final binary for the target
hardware.

\textbf{Reduce measuring overhead:} There were some overhead on the measurement
results caused by the current implementation of the breakpoint instruction.
There is a theoretical fix for this problem on the stable release channel in
the Cortex-M library by adding some additional flags to the Rust compiler and
enabling linker plugin based LTO (link time optimization) when building the
replay harness. Then the final code optimizations will be performed during
linking and not before. This could reduce the overhead drastically by inlining
the instructions. 

This was tested very hastily without any attention to any details, but did not
produce any notable changes. There might have been some missing compilations
flags and other settings that needs to be set in order to optimize it
correctly. Or maybe some modifications needs to be made on the custom fork of
the Cortex-M library for it to work. Either way, this is considered a high
priority to fix in the next iteration of RAUK.

\textbf{Compare with other tools:} Unfortunately the schedulability analysis
was only executed with RAUK and no other available tools were tested. A
comparison with other tools would have been valuable to determine whether the
results from the different tools would differ or not. 

\textbf{Easier maintainability:} To make this tool viable for continued support
in the future, the tool would benefit if many of the libraries in the RTIC and
Cortex-M ecosystem for Rust were more modularized. Many of the forked libraries
are of larger libraries and only changes small parts of them. If only smaller
parts could be forked it would significantly simplify the process of
maintaining them. There currently is work being made on modularizing the RTIC
library itself, and then the harnesses needed by RAUK could be easily extended
in the RTIC library without the need of maintaining the rest of the components.

\textbf{Full RTIC support:} the current implementation of RAUK only supports
a subset of all features in the RTIC framework. Support for analyzing complex
resource types (such as data structures) and full support for software tasks
should also be included in the measuring stage. The current approach for the APIs
used by software tasks to schedule and spawn tasks at certain times using
monotonic timers, is to just ignore the implementation completely during
measurement, thus the result will not comparable to the actual execution time.
